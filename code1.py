# -*- coding: utf-8 -*-
"""code1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1J3DTJ49BgdeQkkkrib3Gmy02xXvlsKN_
"""

# Mini Project: Large Language Models are Zero-Shot Classifiers
# Flowchart used: LLMs Flow (Data Collection -> LLMs -> Classification Results)
# Dataset: COVID-19 Tweets (Kaggle)

# Install necessary libraries (assuming you're in a Jupyter/Colab environment)
!pip install transformers torch pandas matplotlib tqdm --quiet

import pandas as pd
import re
from tqdm import tqdm
from transformers import pipeline
import matplotlib.pyplot as plt

# ---

# # Step 1: Data Collection (Load Dataset)
# NOTE: You will need to have 'covid19_tweets.csv' in your working directory.
try:
    data = pd.read_csv("covid19_tweets.csv")
    print("Dataset loaded successfully!")
    print(f"Total records: {len(data)}")
    # Assuming 'text' is the column containing the tweet content
    print(data.head())
except FileNotFoundError:
    print("Error: 'covid19_tweets.csv' not found. Please ensure the file is in the correct directory.")
    exit()

# ---

# # Step 2: Minimal Preprocessing (Optional for LLMs but good practice)
# LLMs (like BART) handle tokenization, stop words, and stemming internally.
# We only perform a basic cleaning to remove extraneous data like URLs.
def clean_tweet_llm(text):
    """
    Cleans the tweet text by only removing URLs, as LLMs handle other steps.
    """
    # Remove URLs (http/https and www)
    text = re.sub(r"http\S+|www\S+", '', str(text))
    return text.strip()

# Apply cleaning function to the original text
data['clean_text'] = data['text'].apply(clean_tweet_llm)

# Drop rows where 'clean_text' is missing or becomes empty after cleaning
data = data.dropna(subset=['clean_text'])

# Take a small sample for demonstration
sample_data = data.head(100)
# ---

# # Step 3: LLMs (Zero-Shot Classification)
# This step represents the "LLMs" box in the simplified flowchart.
# Initialize the Zero-Shot Classification pipeline
classifier = pipeline("zero-shot-classification", model="facebook/bart-large-mnli")
candidate_labels = ["positive", "negative", "neutral"]

predicted_labels = []
# Iterate over the 'clean_text' column of the sample data with a progress bar (tqdm)
for text in tqdm(sample_data['clean_text'], desc='Classifying'):
    # The zero-shot classification uses the LLM to perform the task
    result = classifier(text, candidate_labels)
    # Append the label with the highest score
    predicted_labels.append(result['labels'][0])

# Add the predicted sentiments back to the sample_data DataFrame
sample_data['predicted_sentiment'] = predicted_labels

# ---

# # Step 4: Classification Results (Visualization & Display)

# Visualization
sentiment_counts = sample_data['predicted_sentiment'].value_counts()

plt.figure(figsize=(6,4))
plt.bar(sentiment_counts.index, sentiment_counts.values)
plt.title("Sentiment Distribution (Zero-Shot Classification)")
plt.xlabel("Sentiment")
plt.ylabel("Count")
plt.show()

# Display Results
print("\nFirst 10 Classified Samples:")
print(sample_data[['clean_text', 'predicted_sentiment']].head(10))

# Step 5: Accuracy & Classification Report (if ground truth exists)
if 'label' in data.columns:
    from sklearn.metrics import accuracy_score, classification_report
    y_true = data['label'].astype(str).str.lower()
    y_pred = data['predicted_sentiment'].astype(str).str.lower()
    print("\nModel Evaluation:")
    print("Accuracy:", round(accuracy_score(y_true, y_pred) * 100, 2), "%")
    print("\nClassification Report:\n", classification_report(y_true, y_pred, digits=3))
else:
    print("\n⚠️ No ground truth labels found — accuracy report skipped.")

from google.colab import files
uploaded = files.upload()

# Mini Project: Large Language Models are Zero-Shot Classifiers
# Flow: Data Collection → LLMs → Classification Results
# Dataset: COVID-19 Tweets (Kaggle)

!pip install transformers torch pandas matplotlib tqdm --quiet

import pandas as pd, re, matplotlib.pyplot as plt
from tqdm import tqdm
from transformers import pipeline

# Step 1: Load Dataset
try:
    # ERROR: FileNotFoundError will occur if 'covid19_tweets.csv' is not found.
    # To fix this, either:
    # 1. Upload the 'covid19_tweets.csv' file to your Colab environment (in the same directory as the notebook).
    # 2. Provide the full path to the file if it's located elsewhere.
    df = pd.read_csv("covid19_tweets.csv")
    print(f"Dataset loaded successfully! Total records: {len(df)}")
except FileNotFoundError:
    # Added a more informative message to the SystemExit
    raise SystemExit("Error: 'covid19_tweets.csv' not found. Please upload the file or provide the correct path.")

# Step 2: Clean Text (Light preprocessing for better LLM accuracy)
df['clean_text'] = df['text'].astype(str).apply(lambda x: re.sub(r"http\S+|www\S+|@\w+|#\w+", '', x).strip())
df = df[df['clean_text'].str.len() > 0].head(100)

# Step 3: Zero-Shot Classification using LLM (BART)
classifier = pipeline("zero-shot-classification", model="facebook/bart-large-mnli")
labels = ["positive", "negative", "neutral"]

def classify(text):
    r = classifier(text, labels)
    # Assign label only if confidence > 0.5, else mark as 'neutral'
    return r['labels'][0] if r['scores'][0] > 0.5 else "neutral"

tqdm.pandas(desc="Classifying")
df['predicted_sentiment'] = df['clean_text'].progress_apply(classify)

# Step 4: Results Visualization
counts = df['predicted_sentiment'].value_counts()
plt.bar(counts.index, counts.values)
plt.title("Sentiment Distribution (Zero-Shot Classification)")
plt.xlabel("Sentiment"); plt.ylabel("Count")
plt.show()

print("\nFirst 10 Classified Samples:")
print(df[['clean_text', 'predicted_sentiment']].head(10))